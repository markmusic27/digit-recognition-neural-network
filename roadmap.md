# Roadmap

> **Note from Mark:** This roadmap was generated by Cursor and walks through how to code up the digit classifier from scratch

A roadmap for implementing a basic neural network to classify handwritten digits from the MNIST dataset using only NumPy and Pandas. You will build everything manually, including forward and backward propagation.



## Prerequisites

- Python programming
- Linear algebra: matrix multiplication, dot product
- Calculus: derivatives, chain rule
- Experience with NumPy and Pandas
- MNIST data available with labels



## Project Structure (Suggested)
- Python programming
- Linear algebra: matrix multiplication, dot product
- Calculus: derivatives, chain rule
- Experience with NumPy and Pandas
- MNIST data available with labels

## Project Structure (Suggested)
```
root/
│
├── data/ # MNIST CSV or image files
├── utils.py # Helper functions (accuracy, plotting, etc.)
├── model.py # Neural network logic
├── train.py # Training script
├── test.py # Evaluation script
└── visualize.ipynb # Analysis and debugging
```

## Step-by-Step Roadmap

### Phase 1: Data Preparation

- [x] Load MNIST data using Pandas or NumPy
- [x] Normalize image pixel values to [0, 1]
- [x] One-hot encode the labels
- [x] Split dataset into training and testing sets



### Phase 2: Forward Propagation

- [x] Define a Neuron (weights, bias, activation)
- [x] Implement activation functions (sigmoid and its derivative)
- [x] Build a Layer class (collection of neurons)
- [x] Implement the forward pass: z = Wx + b, a = activation(z)
- [x] Stack layers to create a multi-layer perceptron
- [x] Implement a prediction method (e.g., argmax output)



### Phase 3: Backpropagation

- [x] Define cost function (MSE)
- [x] Compute gradient of cost w.r.t. output activations
- [x] Apply chain rule to propagate error backward
- [x] Derive gradients w.r.t. weights and biases
- [x] Implement parameter updates using gradient descent



### Phase 4: Training Loop

- [x] Perform forward pass
- [x] Compute loss
- [x] Perform backward pass (backpropagation)
- [x] Update weights and biases
- [x] Track loss and accuracy during training



### Phase 5: Evaluation

re- [ ] Evaluate accuracy on the test dataset
- [ ] Analyze misclassified digits
- [ ] (Optional) Display confusion matrix



## Optional Extensions

- [x] Train with multiple hidden layers
- [x] Visualize training curves (loss and accuracy)